{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<transformers.tokenization_bert.BertTokenizer at 0x7fd6041dd750>,\n",
       " ['选择珠江花园的原因就是方便。',\n",
       "  '笔记本的键盘确实爽。',\n",
       "  '房间太小。其他的都一般。',\n",
       "  '今天才知道这书还有第6卷,真有点郁闷.',\n",
       "  '机器背面似乎被撕了张什么标签，残胶还在。'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载预训练字典和分词方法\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    ")\n",
    "\n",
    "sents = [\n",
    "    '选择珠江花园的原因就是方便。',\n",
    "    '笔记本的键盘确实爽。',\n",
    "    '房间太小。其他的都一般。',\n",
    "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
    "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
    "]\n",
    "\n",
    "tokenizer, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertTokenizer._tokenize() got an unexpected keyword argument 'truncation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#编码两个句子\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m out \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m      3\u001b[0m     text\u001b[39m=\u001b[39;49msents[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     text_pair\u001b[39m=\u001b[39;49msents[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[39m#当句子长度大于max_length时,截断\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m     truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m     \u001b[39m#一律补pad到max_length长度\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     12\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(out)\n\u001b[1;32m     18\u001b[0m tokenizer\u001b[39m.\u001b[39mdecode(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:728\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ids_or_pair_ids, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    727\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m \u001b[39melif\u001b[39;00m is_split_into_words \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ids_or_pair_ids[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    729\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:786\u001b[0m, in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m batch_outputs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    784\u001b[0m \u001b[39mfor\u001b[39;00m first_ids, second_ids \u001b[39min\u001b[39;00m batch_ids_pairs:\n\u001b[1;32m    785\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[0;32m--> 786\u001b[0m         first_ids,\n\u001b[1;32m    787\u001b[0m         second_ids,\n\u001b[1;32m    788\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m    789\u001b[0m         padding\u001b[39m=\u001b[39mPaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\u001b[39m.\u001b[39mvalue,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[1;32m    791\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m    792\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[1;32m    793\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         return_attention_mask\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    796\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    797\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    798\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[1;32m    799\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[1;32m    800\u001b[0m         prepend_batch_axis\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    801\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    802\u001b[0m     )\n\u001b[1;32m    804\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m outputs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    805\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m batch_outputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:778\u001b[0m, in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n\u001b[1;32m    757\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_prepare_for_model\u001b[39m(\n\u001b[1;32m    758\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    772\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    774\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m    Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39m    adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m    manages a moving window (with user defined stride) for overflowing tokens\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \n\u001b[1;32m    779\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m        batch_ids_pairs: list of tokenized input ids or input ids pairs\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     batch_outputs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    784\u001b[0m     \u001b[39mfor\u001b[39;00m first_ids, second_ids \u001b[39min\u001b[39;00m batch_ids_pairs:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:644\u001b[0m, in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:644\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: BertTokenizer._tokenize() got an unexpected keyword argument 'truncation'"
     ]
    }
   ],
   "source": [
    "#编码两个句子\n",
    "out = tokenizer.encode(\n",
    "    text=sents[0],\n",
    "    text_pair=sents[1],\n",
    "\n",
    "    #当句子长度大于max_length时,截断\n",
    "    truncation=True,\n",
    "\n",
    "    #一律补pad到max_length长度\n",
    "    padding='max_length',\n",
    "    add_special_tokens=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None,\n",
    ")\n",
    "\n",
    "print(out)\n",
    "\n",
    "tokenizer.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<transformers.tokenization_bert.BertTokenizer at 0x7fd60e739a90>,\n",
       " ['你好啊', '会不会把'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,sentens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BertTokenizer._tokenize() got an unexpected keyword argument 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m      2\u001b[0m     text\u001b[39m=\u001b[39;49msentens[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      3\u001b[0m     text_pair\u001b[39m=\u001b[39;49msentens[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax_length\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     add_special_token\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      6\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     return_tensors\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:728\u001b[0m, in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ids_or_pair_ids, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    727\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 728\u001b[0m \u001b[39melif\u001b[39;00m is_split_into_words \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(ids_or_pair_ids[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    729\u001b[0m     ids, pair_ids \u001b[39m=\u001b[39m ids_or_pair_ids, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:786\u001b[0m, in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, max_length, stride, truncation_strategy, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m batch_outputs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    784\u001b[0m \u001b[39mfor\u001b[39;00m first_ids, second_ids \u001b[39min\u001b[39;00m batch_ids_pairs:\n\u001b[1;32m    785\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[0;32m--> 786\u001b[0m         first_ids,\n\u001b[1;32m    787\u001b[0m         second_ids,\n\u001b[1;32m    788\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m    789\u001b[0m         padding\u001b[39m=\u001b[39mPaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\u001b[39m.\u001b[39mvalue,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation_strategy\u001b[39m.\u001b[39mvalue,\n\u001b[1;32m    791\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[1;32m    792\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[1;32m    793\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         return_attention_mask\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,  \u001b[39m# we pad in batch afterward\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    796\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    797\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    798\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[1;32m    799\u001b[0m         return_tensors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,  \u001b[39m# We convert the whole batch to tensors at the end\u001b[39;00m\n\u001b[1;32m    800\u001b[0m         prepend_batch_axis\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    801\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    802\u001b[0m     )\n\u001b[1;32m    804\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m outputs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    805\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m batch_outputs:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:778\u001b[0m, in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39m@add_end_docstrings\u001b[39m(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n\u001b[1;32m    757\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_batch_prepare_for_model\u001b[39m(\n\u001b[1;32m    758\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    772\u001b[0m     verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    774\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[39m    Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[39m    adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m    manages a moving window (with user defined stride) for overflowing tokens\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m \n\u001b[1;32m    779\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39m        batch_ids_pairs: list of tokenized input ids or input ids pairs\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     batch_outputs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    784\u001b[0m     \u001b[39mfor\u001b[39;00m first_ids, second_ids \u001b[39min\u001b[39;00m batch_ids_pairs:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:644\u001b[0m, in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai/lib/python3.11/site-packages/transformers/tokenization_utils.py:644\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             )\n\u001b[1;32m    640\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 644\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtransformers.PreTrainedTokenizerFast. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMore information on available tokenizers at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    649\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    650\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: BertTokenizer._tokenize() got an unexpected keyword argument 'padding'"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\n",
    "    text=sentens[0],\n",
    "    text_pair=sentens[1],\n",
    "    padding='max_length',\n",
    "    add_special_token=True,\n",
    "    max_length=30,\n",
    "    return_tensors=None,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
